<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[CrowdChat Engineering]]></title>
  <link href="http://crowdchat.github.io/atom.xml" rel="self"/>
  <link href="http://crowdchat.github.io/"/>
  <updated>2013-11-06T22:08:49-08:00</updated>
  <id>http://crowdchat.github.io/</id>
  <author>
    <name><![CDATA[CrowdChat Engineering]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Identifying Real Humans on Twitter]]></title>
    <link href="http://crowdchat.github.io/blog/2013/11/06/identifying-real-humans-on-twitter/"/>
    <updated>2013-11-06T16:58:00-08:00</updated>
    <id>http://crowdchat.github.io/blog/2013/11/06/identifying-real-humans-on-twitter</id>
    <content type="html"><![CDATA[<h1>EM Algorithm</h1>

<p>Let’s recall the naive Bayes algorithm: given a tweet (a set of character n-grams), we estimate its language to be the language L that maximizes</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>P(language=L|ngrams)∝P(ngrams|language=L)P(language=L)
</span><span class='line'>Thus, we need to estimate P(ngram|language=L) and P(language=L).</span></code></pre></td></tr></table></div></figure>


<p>This would be easy if we knew the language of each tweet, since we could estimate</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>P(xyz|language=English) as #(number of times “xyz” is a trigram in the English tweets) / #(total trigrams in the English tweets)
</span><span class='line'>P(language=English) as the proportion of English tweets.
</span></code></pre></td></tr></table></div></figure>


<p>Or, it would also be easy if we knew the n-gram probabilities for each language, since we could use Bayes’ theorem to compute the language probabilities for each tweet, and then take a weighted variant of the previous paragraph.</p>
]]></content>
  </entry>
  
</feed>
